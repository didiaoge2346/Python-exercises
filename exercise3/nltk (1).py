# -*- coding: utf-8 -*-
"""nltk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4TpkjfOG4ZI6QTkUXy2JBZnppi4dIAA
"""

import os
import nltk
import requests
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer

# 下载 NLTK 数据包和停用词列表
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('vader_lexicon')

# 检查文件是否存在，如果不存在，则从URL下载它
file_path = "MobyDick.txt"
url = "https://www.gutenberg.org/files/2701/2701-0.txt"

if not os.path.exists(file_path):
    response = requests.get(url)
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(response.text)

with open(file_path, 'r', encoding='utf-8') as file:
    moby_dick_text = file.read()

# 任务1：分词
tokens = word_tokenize(moby_dick_text)

# 任务2：去除停用词
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

# 任务3：词性标注
pos_tagged_tokens = nltk.pos_tag(filtered_tokens)

# 任务4：词性频率
pos_freq = FreqDist(tag for (word, tag) in pos_tagged_tokens)
most_common_pos = pos_freq.most_common(5)

# 任务5：词形还原
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [(lemmatizer.lemmatize(word, pos='v'), tag) for word, tag in pos_tagged_tokens[:20]]

# 任务6：绘制频率分布图
pos_freq.plot(30, cumulative=False)
plt.show()

# 可选任务：情感分析
sia = SentimentIntensityAnalyzer()
sentiment_score = sia.polarity_scores(moby_dick_text)['compound']

# 显示结果
print("5 most common parts of speech with their frequencies:", most_common_pos)
print("Top 20 lemmatized tokens:", lemmatized_tokens)
print("Sentiment score:", sentiment_score)
print("Overall sentiment:", "Positive" if sentiment_score > 0.05 else "Negative")